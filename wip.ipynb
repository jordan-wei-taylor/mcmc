{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMCSampler():\n",
    "\n",
    "    def __init__(self, log_pstar, covariance, data, **kwargs):\n",
    "        self.log_pstar  = log_pstar\n",
    "        self.covariance = covariance\n",
    "        self.data       = data\n",
    "        self.kwargs     = kwargs\n",
    "\n",
    "    def transition(self, theta):\n",
    "        return stats.multivariate_normal(theta, self.covariance).rvs()\n",
    "\n",
    "    def get_samples(self, burn_period = 0.2):\n",
    "        if isinstance(burn_period, float):\n",
    "            burn_period = int(burn_period * len(self.samples) + 1)\n",
    "        return self.samples[burn_period:]\n",
    "\n",
    "def check_verbose(N, verbose):\n",
    "    if isinstance(verbose, str):\n",
    "        if verbose == 'auto':\n",
    "            temp = 10 ** np.floor(np.log10(N) - 1)\n",
    "            k    = np.array([1, 2, 5])\n",
    "            arg  = np.fabs(N // temp / k - 10).argmin()\n",
    "            verbose = int(temp * k[arg])\n",
    "        else:\n",
    "            raise Exception()\n",
    "    elif isinstance(verbose, str):\n",
    "        verbose = int(N * verbose + 0.5)\n",
    "\n",
    "    assert isinstance(verbose, int)\n",
    "    assert 0 < verbose < N\n",
    "\n",
    "    return verbose\n",
    "\n",
    "def softmax(z):\n",
    "    e = np.exp(z - z.max(axis = -1, keepdims = True)) # numerical stability\n",
    "    return e / e.sum(axis = -1, keepdims = True)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class Verbose():\n",
    "\n",
    "    def __init__(self, N, verbose):\n",
    "        self.N = N\n",
    "        self.verbose = verbose\n",
    "        self.num = max(len(f'{N:,d}'), len('iteration'))\n",
    "\n",
    "        space = max(len('iteration') - self.num, 0)\n",
    "\n",
    "        print(' ' * space + 'iteration | log pstar')\n",
    "        print('-' * space + '----------+----------')\n",
    "\n",
    "    def print(self, i, val, end = '\\n'):\n",
    "        print(f'\\r{i:>{self.num},d} | {val:+.2e}', end = end)\n",
    "\n",
    "class MetropolisHastingsSampler(MCMCSampler):\n",
    "\n",
    "    def __init__(self, log_pstar, covariance, data, **kwargs):\n",
    "        super().__init__(log_pstar, covariance, data, **kwargs)\n",
    "\n",
    "    def fit(self, n_samples, theta0, verbose = 'auto', random_state = None):\n",
    "\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "        N                  = n_samples + 1 # add 1 to include theta0\n",
    "        m                  = len(theta0)   # no. of parameters\n",
    "        \n",
    "        assert n_samples > 0\n",
    "        assert isinstance(theta0, np.ndarray) and (theta0.ndim == 1)\n",
    "\n",
    "        verbose            = check_verbose(N, verbose)\n",
    "\n",
    "        # samples\n",
    "        self.samples       = np.empty((N, m))\n",
    "        self.samples[0]    = theta0\n",
    "\n",
    "        # record of all log_pstar evaluations\n",
    "        self.log_pstars    = np.empty(N)\n",
    "        self.log_pstars[0] = self.log_pstar(theta0, self.data, **self.kwargs)\n",
    "\n",
    "        # acceptance indicator for all new samples\n",
    "        self.acceptance    = np.zeros(N - 1, dtype = bool)\n",
    "\n",
    "        if verbose:\n",
    "            message = Verbose(N, verbose)\n",
    "            message.print(0, self.log_pstars[0])\n",
    "\n",
    "        for i in range(1, N):\n",
    "\n",
    "            # sample a new theta\n",
    "            theta = self.transition(self.samples[i - 1])\n",
    "\n",
    "            # compute log pstar of new theta\n",
    "            logp  = self.log_pstar(theta, self.data, **self.kwargs)\n",
    "\n",
    "            # accept with p_new / p_old probability\n",
    "            if np.log(np.random.uniform()) < (logp - self.log_pstars[i - 1]):\n",
    "                self.samples[i]        = theta\n",
    "                self.log_pstars[i]     = logp\n",
    "                self.acceptance[i - 1] = True\n",
    "\n",
    "            # reject and add the previous sample\n",
    "            else:\n",
    "                self.samples[i]        = self.samples[i - 1]\n",
    "                self.log_pstars[i]     = self.log_pstars[i - 1]\n",
    "            \n",
    "            if verbose:\n",
    "                message.print(i, self.log_pstars[i], '' if i % verbose else '\\n')\n",
    "\n",
    "        return self\n",
    "\n",
    "class GibbsSampler(MCMCSampler):\n",
    "\n",
    "    def __init__(self, log_pstar, deviation, data, **kwargs):\n",
    "        super().__init__(log_pstar, deviation, data, **kwargs)\n",
    "\n",
    "        self.__dict__['deviation'] = self.__dict__.pop('covariance')\n",
    "\n",
    "    def transition(self, value, j):\n",
    "        return stats.norm(value, self.deviation[j]).rvs()\n",
    "\n",
    "    def fit(self, n_samples, theta0, verbose = 'auto', random_state = None):\n",
    "\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "        N                  = n_samples + 1 # add 1 to include theta0\n",
    "        m                  = len(theta0)   # no. of parameters\n",
    "\n",
    "        assert n_samples > 0\n",
    "        assert isinstance(theta0, np.ndarray) and (theta0.ndim == 1)\n",
    "\n",
    "        verbose = check_verbose(N, verbose)\n",
    "\n",
    "        # samples\n",
    "        self.samples       = np.empty((N, m))\n",
    "        self.samples[0]    = theta0\n",
    "\n",
    "        # record of all log_pstar evaluations\n",
    "        self.log_pstars    = np.empty(N)\n",
    "        self.log_pstars[0] = self.log_pstar(theta0, self.data, **self.kwargs)\n",
    "\n",
    "        # acceptance rate indicator for all new samples (per parameter)\n",
    "        self.acceptance    = np.zeros((N - 1, m), dtype = bool)\n",
    "\n",
    "        if verbose:\n",
    "            message = Verbose(N, verbose)\n",
    "            message.print(0, self.log_pstars[0])\n",
    "\n",
    "        for i in range(1, N):\n",
    "\n",
    "            # theta and logp baseline\n",
    "            theta_baseline = self.samples[i - 1]\n",
    "            logp_baseline  = self.log_pstars[i - 1]\n",
    "\n",
    "            # loop through each parameter in theta\n",
    "            for j in range(m):\n",
    "                \n",
    "                # copy most recent theta baseline\n",
    "                theta    = theta_baseline.copy()\n",
    "\n",
    "                # sample the j-th element\n",
    "                theta[j] = self.transition(theta[j], j)\n",
    "                \n",
    "                # compute log_pstar for theta\n",
    "                logp     = self.log_pstar(theta, self.data, **self.kwargs)\n",
    "\n",
    "                # accept with p_new / p_old probability\n",
    "                if np.log(np.random.uniform()) < (logp - logp_baseline):\n",
    "                    theta_baseline = theta\n",
    "                    logp_baseline  = logp\n",
    "                \n",
    "                # reject the new sample\n",
    "                else:\n",
    "                    self.acceptance[i - 1,j] = False\n",
    "                \n",
    "            # append new sample and log_pstar values\n",
    "            self.samples[i]    = theta_baseline\n",
    "            self.log_pstars[i] = logp_baseline\n",
    "            \n",
    "            if verbose:\n",
    "                message.print(i, self.log_pstars[i], '' if i % verbose else '\\n')\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class AdaptiveGibbsSampler(MCMCSampler):\n",
    "\n",
    "    def __init__(self, log_pstar, deviation, data, rate = 0.8, **kwargs):\n",
    "\n",
    "        assert isinstance(rate, float) and 0 < rate < 1\n",
    "\n",
    "        super().__init__(log_pstar, deviation, data, **kwargs)\n",
    "\n",
    "        self.__dict__['deviation'] = self.__dict__.pop('covariance')\n",
    "        self.rate = rate\n",
    "\n",
    "    def transition(self, value, j):\n",
    "        return stats.norm(value, self.deviation[j]).rvs()\n",
    "\n",
    "    def fit(self, n_samples, theta0, verbose = 'auto', random_state = None):\n",
    "\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "        N                  = n_samples + 1 # add 1 to include theta0\n",
    "        m                  = len(theta0)   # no. of parameters\n",
    "\n",
    "        assert n_samples > 0\n",
    "        assert isinstance(theta0, np.ndarray) and (theta0.ndim == 1)\n",
    "\n",
    "        verbose = check_verbose(N, verbose)\n",
    "\n",
    "        # samples\n",
    "        self.samples       = np.empty((N, m))\n",
    "        self.samples[0]    = theta0\n",
    "\n",
    "        # record of all log_pstar evaluations\n",
    "        self.log_pstars    = np.empty(N)\n",
    "        self.log_pstars[0] = self.log_pstar(theta0, self.data, **self.kwargs)\n",
    "\n",
    "        # acceptance rate indicator for all new samples\n",
    "        self.acceptance    = {}\n",
    "\n",
    "        # log_alpha for the dirichlet prior\n",
    "        log_alpha          = np.zeros(m)\n",
    "\n",
    "        if verbose:\n",
    "            message = Verbose(N, verbose)\n",
    "            message.print(0, self.log_pstars[0])\n",
    "\n",
    "        for i in range(1, N):\n",
    "            \n",
    "            # theta and logp baseline\n",
    "            theta_baseline = self.samples[i - 1]\n",
    "            logp_baseline  = self.log_pstars[i - 1]\n",
    "\n",
    "            # initialise the acceptance rate for the i-th sample to be a dictionary with list values\n",
    "            self.acceptance[i] = defaultdict(list)\n",
    "\n",
    "            for _ in range(m):\n",
    "                \n",
    "                # compute alpha\n",
    "                alpha    = softmax(log_alpha) + 1e-8             # numerical stability as alpha needs to be greater than 0\n",
    "\n",
    "                # sample the j-th parameter to update\n",
    "                j        = stats.dirichlet(alpha).rvs().argmax() # draws m numbers but select the one with the highest value\n",
    "\n",
    "                # copy most recent theta baseline\n",
    "                theta    = theta_baseline.copy()\n",
    "\n",
    "                # sample the j-th element\n",
    "                theta[j] = self.transition(theta[j], j)\n",
    "                \n",
    "                # compute log_pstar for theta\n",
    "                logp     = self.log_pstar(theta, self.data, **self.kwargs)\n",
    "\n",
    "                # accept with probability p_new / p_old\n",
    "                if np.log(np.random.uniform()) < (logp - logp_baseline):\n",
    "                    \n",
    "                    # increment the j-th element of log_alpha by the improvement in log_pstar value\n",
    "                    log_alpha[j]  += logp - logp_baseline\n",
    "\n",
    "                    theta_baseline = theta\n",
    "                    logp_baseline  = logp\n",
    "                    self.acceptance[i][j].append(True)\n",
    "                    \n",
    "                # reject the new sample\n",
    "                else:\n",
    "                    self.acceptance[i][j].append(False)\n",
    "\n",
    "                # pull all values towards 0 (this prevents exploding values)\n",
    "                # encourages indices that have not been picked\n",
    "                log_alpha *= self.rate\n",
    "                \n",
    "            self.samples[i]    = theta_baseline\n",
    "            self.log_pstars[i] = logp_baseline\n",
    "            \n",
    "            if verbose:\n",
    "                message.print(i, self.log_pstars[i], '' if i % verbose else '\\n')\n",
    "                \n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y = True)\n",
    "    \n",
    "def log_pstar_classification(theta, data, **kwargs):\n",
    "    X, y = data\n",
    "    p    = len(np.unique(y))\n",
    "    bw   = theta.reshape(-1, p)\n",
    "    b, w = bw[0], bw[1:]\n",
    "    hat  = softmax(X @ w + b)\n",
    "    return np.log(hat[range(len(hat)), y]).sum()\n",
    "\n",
    "def log_pstar_binary_classification(theta, data, **kwargs):\n",
    "    X, y = data\n",
    "    bw   = theta.reshape(X.shape[1] + 1)\n",
    "    b, w = bw[0], bw[1:]\n",
    "    hat  = sigmoid(X @ w + b)\n",
    "    return np.log(hat[y == 1]).sum() - np.log(1 - hat[y == 0]).sum()\n",
    "\n",
    "def log_pstar_regression(theta, data, noise_deviation = 0.2, **kwargs):\n",
    "    X, y = data\n",
    "    b, w = theta[0], theta[1:]\n",
    "    return stats.norm(X @ w + b, noise_deviation).logpdf(y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N         = 200\n",
    "theta0    = np.zeros((X.shape[1] * 10 + 10))\n",
    "deviation = np.ones_like(theta0) * 0.1 # increase to 0.1\n",
    "data      = (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration | log pstar\n",
      "----------+----------\n",
      "        0 | -4.14e+03\n",
      "       20 | -4.14e+03\n",
      "       21 | -4.14e+03"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m mh \u001b[38;5;241m=\u001b[39m MetropolisHastingsSampler(log_pstar_classification, deviation, data)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta0\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [2], line 92\u001b[0m, in \u001b[0;36mMetropolisHastingsSampler.fit\u001b[0;34m(self, n_samples, theta0, verbose, random_state)\u001b[0m\n\u001b[1;32m     87\u001b[0m     message\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_pstars[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, N):\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# sample a new theta\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     theta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# compute log pstar of new theta\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     logp  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_pstar(theta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn [2], line 10\u001b[0m, in \u001b[0;36mMCMCSampler.transition\u001b[0;34m(self, theta)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransition\u001b[39m(\u001b[38;5;28mself\u001b[39m, theta):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariance\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrvs()\n",
      "File \u001b[0;32m~/git-workspace/mcmc/env/lib/python3.10/site-packages/scipy/stats/_multivariate.py:364\u001b[0m, in \u001b[0;36mmultivariate_normal_gen.__call__\u001b[0;34m(self, mean, cov, allow_singular, seed)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, mean\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cov\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, allow_singular\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, seed\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m     \u001b[39m\"\"\"Create a frozen multivariate normal distribution.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[1;32m    362\u001b[0m \u001b[39m    See `multivariate_normal_frozen` for more information.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m multivariate_normal_frozen(mean, cov,\n\u001b[1;32m    365\u001b[0m                                       allow_singular\u001b[39m=\u001b[39;49mallow_singular,\n\u001b[1;32m    366\u001b[0m                                       seed\u001b[39m=\u001b[39;49mseed)\n",
      "File \u001b[0;32m~/git-workspace/mcmc/env/lib/python3.10/site-packages/scipy/stats/_multivariate.py:734\u001b[0m, in \u001b[0;36mmultivariate_normal_frozen.__init__\u001b[0;34m(self, mean, cov, allow_singular, seed, maxpts, abseps, releps)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dist \u001b[39m=\u001b[39m multivariate_normal_gen(seed)\n\u001b[1;32m    732\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dist\u001b[39m.\u001b[39m_process_parameters(\n\u001b[1;32m    733\u001b[0m                                                     \u001b[39mNone\u001b[39;00m, mean, cov)\n\u001b[0;32m--> 734\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_info \u001b[39m=\u001b[39m _PSD(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcov, allow_singular\u001b[39m=\u001b[39;49mallow_singular)\n\u001b[1;32m    735\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m maxpts:\n\u001b[1;32m    736\u001b[0m     maxpts \u001b[39m=\u001b[39m \u001b[39m1000000\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim\n",
      "File \u001b[0;32m~/git-workspace/mcmc/env/lib/python3.10/site-packages/scipy/stats/_multivariate.py:157\u001b[0m, in \u001b[0;36m_PSD.__init__\u001b[0;34m(self, M, cond, rcond, lower, check_finite, allow_singular)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, M, cond\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, rcond\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lower\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    153\u001b[0m              check_finite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_singular\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    154\u001b[0m     \u001b[39m# Compute the symmetric eigendecomposition.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[39m# Note that eigh takes care of array conversion, chkfinite,\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39m# and assertion that the matrix is square.\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     s, u \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49meigh(M, lower\u001b[39m=\u001b[39;49mlower, check_finite\u001b[39m=\u001b[39;49mcheck_finite)\n\u001b[1;32m    159\u001b[0m     eps \u001b[39m=\u001b[39m _eigvalsh_to_eps(s, cond, rcond)\n\u001b[1;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mmin(s) \u001b[39m<\u001b[39m \u001b[39m-\u001b[39meps:\n",
      "File \u001b[0;32m~/git-workspace/mcmc/env/lib/python3.10/site-packages/scipy/linalg/_decomp.py:547\u001b[0m, in \u001b[0;36meigh\u001b[0;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite, subset_by_index, subset_by_value, driver)\u001b[0m\n\u001b[1;32m    544\u001b[0m         lwork_args \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlwork\u001b[39m\u001b[39m'\u001b[39m: lw}\n\u001b[1;32m    546\u001b[0m     drv_args\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mlower\u001b[39m\u001b[39m'\u001b[39m: lower, \u001b[39m'\u001b[39m\u001b[39mcompute_v\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m _job \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mN\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m})\n\u001b[0;32m--> 547\u001b[0m     w, v, \u001b[39m*\u001b[39mother_args, info \u001b[39m=\u001b[39m drv(a\u001b[39m=\u001b[39;49ma1, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdrv_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mlwork_args)\n\u001b[1;32m    549\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Generalized problem\u001b[39;00m\n\u001b[1;32m    550\u001b[0m     \u001b[39m# 'gvd' doesn't have lwork query\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     \u001b[39mif\u001b[39;00m driver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgvd\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mh = MetropolisHastingsSampler(log_pstar_classification, deviation, data)\n",
    "\n",
    "mh.fit(N, theta0)\n",
    "\n",
    "# compare the average of the samples to the true values\n",
    "# print(mcmc.get_samples(500).mean(axis = 0), np.append(b, w))\n",
    "\n",
    "# # norm distance\n",
    "# np.linalg.norm(mcmc.get_samples(500).mean(axis = 0) - np.append(b, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration | log pstar\n",
      "----------+----------\n",
      "        0 | -4.14e+03\n",
      "       20 | -2.04e+02\n",
      "       40 | -1.34e+02\n",
      "       60 | -1.07e+02\n",
      "       80 | -1.01e+02\n",
      "      100 | -8.69e+01\n",
      "      120 | -8.50e+01\n",
      "      140 | -7.70e+01\n",
      "      160 | -6.04e+01\n",
      "      180 | -6.72e+01\n",
      "      200 | -7.20e+01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GibbsSampler at 0x7f1964c5e890>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gibbs = GibbsSampler(log_pstar_classification, deviation, data)\n",
    "\n",
    "gibbs.fit(N, theta0)\n",
    "\n",
    "# compare the average of the samples to the true values\n",
    "# print(mcmc.get_samples(500).mean(axis = 0), np.append(b, w))\n",
    "\n",
    "# # norm distance\n",
    "# np.linalg.norm(mcmc.get_samples(500).mean(axis = 0) - np.append(b, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration | log pstar\n",
      "----------+----------\n",
      "        0 | -4.14e+03\n",
      "       20 | -2.12e+02\n",
      "       25 | -1.83e+02"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m adaptive \u001b[38;5;241m=\u001b[39m AdaptiveGibbsSampler(log_pstar_classification, deviation, data)\n\u001b[0;32m----> 3\u001b[0m \u001b[43madaptive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta0\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [2], line 247\u001b[0m, in \u001b[0;36mAdaptiveGibbsSampler.fit\u001b[0;34m(self, n_samples, theta0, verbose, random_state)\u001b[0m\n\u001b[1;32m    244\u001b[0m theta    \u001b[38;5;241m=\u001b[39m theta_baseline\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# sample the j-th element\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m theta[j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# compute log_pstar for theta\u001b[39;00m\n\u001b[1;32m    250\u001b[0m logp     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_pstar(theta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn [2], line 193\u001b[0m, in \u001b[0;36mAdaptiveGibbsSampler.transition\u001b[0;34m(self, value, j)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransition\u001b[39m(\u001b[38;5;28mself\u001b[39m, value, j):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeviation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrvs()\n",
      "File \u001b[0;32m~/git-workspace/mcmc/env/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py:859\u001b[0m, in \u001b[0;36mrv_generic.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m--> 859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfreeze(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/git-workspace/mcmc/env/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py:854\u001b[0m, in \u001b[0;36mrv_generic.freeze\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[39m\"\"\"Freeze the distribution for the given arguments.\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \n\u001b[1;32m    841\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    851\u001b[0m \n\u001b[1;32m    852\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, rv_continuous):\n\u001b[0;32m--> 854\u001b[0m     \u001b[39mreturn\u001b[39;00m rv_continuous_frozen(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    855\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    856\u001b[0m     \u001b[39mreturn\u001b[39;00m rv_discrete_frozen(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/git-workspace/mcmc/env/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py:439\u001b[0m, in \u001b[0;36mrv_frozen.__init__\u001b[0;34m(self, dist, *args, **kwds)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds \u001b[39m=\u001b[39m kwds\n\u001b[1;32m    438\u001b[0m \u001b[39m# create a new instance\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdist\u001b[39m.\u001b[39;49m_updated_ctor_param())\n\u001b[1;32m    441\u001b[0m shapes, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist\u001b[39m.\u001b[39m_parse_args(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist\u001b[39m.\u001b[39m_get_support(\u001b[39m*\u001b[39mshapes)\n",
      "File \u001b[0;32m~/git-workspace/mcmc/env/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py:1928\u001b[0m, in \u001b[0;36mrv_continuous.__init__\u001b[0;34m(self, momtype, a, b, xtol, badvalue, name, longname, shapes, extradoc, seed)\u001b[0m\n\u001b[1;32m   1923\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextradoc \u001b[39m=\u001b[39m extradoc\n\u001b[1;32m   1925\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_argparser(meths_to_inspect\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cdf],\n\u001b[1;32m   1926\u001b[0m                           locscale_in\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloc=0, scale=1\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1927\u001b[0m                           locscale_out\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloc, scale\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1928\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attach_methods()\n\u001b[1;32m   1930\u001b[0m \u001b[39mif\u001b[39;00m longname \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1931\u001b[0m     \u001b[39mif\u001b[39;00m name[\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39maeiouAEIOU\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[0;32m~/git-workspace/mcmc/env/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py:1963\u001b[0m, in \u001b[0;36mrv_continuous._attach_methods\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1959\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1960\u001b[0m \u001b[39mAttaches dynamically created methods to the rv_continuous instance.\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1962\u001b[0m \u001b[39m# _attach_methods is responsible for calling _attach_argparser_methods\u001b[39;00m\n\u001b[0;32m-> 1963\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attach_argparser_methods()\n\u001b[1;32m   1965\u001b[0m \u001b[39m# nin correction\u001b[39;00m\n\u001b[1;32m   1966\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ppfvec \u001b[39m=\u001b[39m vectorize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ppf_single, otypes\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/git-workspace/mcmc/env/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py:699\u001b[0m, in \u001b[0;36mrv_generic._attach_argparser_methods\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[39mGenerates the argument-parsing functions dynamically and attaches\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[39mthem to the instance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mduring unpickling (__setstate__)\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    698\u001b[0m ns \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 699\u001b[0m exec(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_arg_template, ns)\n\u001b[1;32m    700\u001b[0m \u001b[39m# NB: attach to the instance, not class\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39m_parse_args\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_parse_args_stats\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_parse_args_rvs\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "adaptive = AdaptiveGibbsSampler(log_pstar_classification, deviation, data)\n",
    "\n",
    "adaptive.fit(N, theta0)\n",
    "\n",
    "# compare the average of the samples to the true values\n",
    "# print(mcmc.get_samples(500).mean(axis = 0), np.append(b, w))\n",
    "\n",
    "# # norm distance\n",
    "# np.linalg.norm(mcmc.get_samples(500).mean(axis = 0) - np.append(b, w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa841b4197c6a393f3c57e5da69b1046a13d3ffb1b2ced83c8f3efb4d967ec88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
