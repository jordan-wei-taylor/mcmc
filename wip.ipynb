{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMCSampler():\n",
    "\n",
    "    def __init__(self, log_pstar, covariance, data, **kwargs):\n",
    "        self.log_pstar  = log_pstar\n",
    "        self.covariance = covariance\n",
    "        self.data       = data\n",
    "        self.kwargs     = kwargs\n",
    "\n",
    "    def transition(self, theta):\n",
    "        return stats.multivariate_normal(theta, self.covariance).rvs()\n",
    "\n",
    "    def get_samples(self, burn_period = 0.2):\n",
    "        if isinstance(burn_period, float):\n",
    "            burn_period = int(burn_period * len(self.samples) + 1)\n",
    "        return self.samples[burn_period:]\n",
    "\n",
    "def check_verbose(N, verbose):\n",
    "    if isinstance(verbose, str):\n",
    "        if verbose == 'auto':\n",
    "            temp = 10 ** np.floor(np.log10(N) - 1)\n",
    "            k    = np.array([1, 2, 5])\n",
    "            arg  = np.fabs(N // temp / k - 10).argmin()\n",
    "            verbose = int(temp * k[arg])\n",
    "        else:\n",
    "            raise Exception()\n",
    "    elif isinstance(verbose, str):\n",
    "        verbose = int(N * verbose + 0.5)\n",
    "\n",
    "    assert isinstance(verbose, int)\n",
    "    assert 0 < verbose < N\n",
    "\n",
    "    return verbose\n",
    "\n",
    "class Verbose():\n",
    "\n",
    "    def __init__(self, N, verbose):\n",
    "        self.N = N\n",
    "        self.verbose = verbose\n",
    "        self.num = max(len(f'{N:,d}'), len('iteration'))\n",
    "\n",
    "        space = max(len('iteration') - self.num, 0)\n",
    "\n",
    "        print(' ' * space + 'iteration | log pstar')\n",
    "        print('-' * space + '----------+----------')\n",
    "\n",
    "    def print(self, i, val, end = '\\n'):\n",
    "        print(f'\\r{i:>{self.num},d} | {val:+.2e}', end = end)\n",
    "\n",
    "class MetropolisHastingsSampler(MCMCSampler):\n",
    "\n",
    "    def __init__(self, log_pstar, covariance, data, **kwargs):\n",
    "        super().__init__(log_pstar, covariance, data, **kwargs)\n",
    "\n",
    "    def fit(self, n_samples, theta0, verbose = 'auto', random_state = None):\n",
    "\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "        N                  = n_samples + 1 # add 1 to include theta0\n",
    "        m                  = len(theta0)   # no. of parameters\n",
    "        \n",
    "        assert n_samples > 0\n",
    "        assert isinstance(theta0, np.ndarray) and (theta0.ndim == 1)\n",
    "\n",
    "        verbose            = check_verbose(N, verbose)\n",
    "\n",
    "        # samples\n",
    "        self.samples       = np.empty((N, m))\n",
    "        self.samples[0]    = theta0\n",
    "\n",
    "        # record of all log_pstar evaluations\n",
    "        self.log_pstars    = np.empty(N)\n",
    "        self.log_pstars[0] = self.log_pstar(theta0, self.data, **self.kwargs)\n",
    "\n",
    "        # acceptance indicator for all new samples\n",
    "        self.acceptance    = np.zeros(N - 1, dtype = bool)\n",
    "\n",
    "        if verbose:\n",
    "            message = Verbose(N, verbose)\n",
    "            message.print(0, self.log_pstars[0])\n",
    "\n",
    "        for i in range(1, N):\n",
    "\n",
    "            # sample a new theta\n",
    "            theta = self.transition(self.samples[i - 1])\n",
    "\n",
    "            # compute log pstar of new theta\n",
    "            logp  = self.log_pstar(theta, self.data, **self.kwargs)\n",
    "\n",
    "            # accept with p_new / p_old probability\n",
    "            if np.log(np.random.uniform()) < (logp - self.log_pstars[i - 1]):\n",
    "                self.samples[i]        = theta\n",
    "                self.log_pstars[i]     = logp\n",
    "                self.acceptance[i - 1] = True\n",
    "\n",
    "            # reject and add the previous sample\n",
    "            else:\n",
    "                self.samples[i]        = self.samples[i - 1]\n",
    "                self.log_pstars[i]     = self.log_pstars[i - 1]\n",
    "            \n",
    "            if verbose:\n",
    "                message.print(i, self.log_pstars[i], '' if i % verbose else '\\n')\n",
    "\n",
    "        return self\n",
    "\n",
    "class GibbsSampler(MCMCSampler):\n",
    "\n",
    "    def __init__(self, log_pstar, deviation, data, **kwargs):\n",
    "        super().__init__(log_pstar, deviation, data, **kwargs)\n",
    "\n",
    "        self.__dict__['deviation'] = self.__dict__.pop('covariance')\n",
    "\n",
    "    def transition(self, value, j):\n",
    "        return stats.norm(value, self.deviation[j]).rvs()\n",
    "\n",
    "    def fit(self, n_samples, theta0, verbose = 'auto', random_state = None):\n",
    "\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "        N                  = n_samples + 1 # add 1 to include theta0\n",
    "        m                  = len(theta0)   # no. of parameters\n",
    "\n",
    "        assert n_samples > 0\n",
    "        assert isinstance(theta0, np.ndarray) and (theta0.ndim == 1)\n",
    "\n",
    "        verbose = check_verbose(N, verbose)\n",
    "\n",
    "        # samples\n",
    "        self.samples       = np.empty((N, m))\n",
    "        self.samples[0]    = theta0\n",
    "\n",
    "        # record of all log_pstar evaluations\n",
    "        self.log_pstars    = np.empty(N)\n",
    "        self.log_pstars[0] = self.log_pstar(theta0, self.data, **self.kwargs)\n",
    "\n",
    "        # acceptance rate indicator for all new samples (per parameter)\n",
    "        self.acceptance    = np.zeros((N - 1, m), dtype = bool)\n",
    "\n",
    "        if verbose:\n",
    "            message = Verbose(N, verbose)\n",
    "            message.print(0, self.log_pstars[0])\n",
    "\n",
    "        for i in range(1, N):\n",
    "\n",
    "            # theta and logp baseline\n",
    "            theta_baseline = self.samples[i - 1]\n",
    "            logp_baseline  = self.log_pstars[i - 1]\n",
    "\n",
    "            # loop through each parameter in theta\n",
    "            for j in range(m):\n",
    "                \n",
    "                # copy most recent theta baseline\n",
    "                theta    = theta_baseline.copy()\n",
    "\n",
    "                # sample the j-th element\n",
    "                theta[j] = self.transition(theta[j], j)\n",
    "                \n",
    "                # compute log_pstar for theta\n",
    "                logp     = self.log_pstar(theta, self.data, **self.kwargs)\n",
    "\n",
    "                # accept with p_new / p_old probability\n",
    "                if np.log(np.random.uniform()) < (logp - logp_baseline):\n",
    "                    theta_baseline = theta\n",
    "                    logp_baseline  = logp\n",
    "                \n",
    "                # reject the new sample\n",
    "                else:\n",
    "                    self.acceptance[i - 1,j] = False\n",
    "                \n",
    "            # append new sample and log_pstar values\n",
    "            self.samples[i]    = theta_baseline\n",
    "            self.log_pstars[i] = logp_baseline\n",
    "            \n",
    "            if verbose:\n",
    "                message.print(i, self.log_pstars[i], '' if i % verbose else '\\n')\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class AdaptiveGibbsSampler(MCMCSampler):\n",
    "\n",
    "    def __init__(self, log_pstar, deviation, data, rate = 0.9, **kwargs):\n",
    "\n",
    "        assert isinstance(rate, float) and 0 < rate < 1\n",
    "\n",
    "        super().__init__(log_pstar, deviation, data, **kwargs)\n",
    "\n",
    "        self.__dict__['deviation'] = self.__dict__.pop('covariance')\n",
    "        self.rate = rate\n",
    "\n",
    "    def transition(self, value, j):\n",
    "        return stats.norm(value, self.deviation[j]).rvs()\n",
    "\n",
    "    def fit(self, n_samples, theta0, verbose = 'auto', random_state = None):\n",
    "\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "        N                  = n_samples + 1 # add 1 to include theta0\n",
    "        m                  = len(theta0)   # no. of parameters\n",
    "\n",
    "        assert n_samples > 0\n",
    "        assert isinstance(theta0, np.ndarray) and (theta0.ndim == 1)\n",
    "\n",
    "        verbose = check_verbose(N, verbose)\n",
    "\n",
    "        # samples\n",
    "        self.samples       = np.empty((N, m))\n",
    "        self.samples[0]    = theta0\n",
    "\n",
    "        # record of all log_pstar evaluations\n",
    "        self.log_pstars    = np.empty(N)\n",
    "        self.log_pstars[0] = self.log_pstar(theta0, self.data, **self.kwargs)\n",
    "\n",
    "        # acceptance rate indicator for all new samples\n",
    "        self.acceptance    = {}\n",
    "\n",
    "        # log_alpha for the dirichlet prior\n",
    "        log_alpha          = np.zeros(m)\n",
    "\n",
    "        if verbose:\n",
    "            message = Verbose(N, verbose)\n",
    "            message.print(0, self.log_pstars[0])\n",
    "\n",
    "        for i in range(1, N):\n",
    "            \n",
    "            # theta and logp baseline\n",
    "            theta_baseline = self.samples[i - 1]\n",
    "            logp_baseline  = self.log_pstars[i - 1]\n",
    "\n",
    "            # initialise the acceptance rate for the i-th sample to be a dictionary with list values\n",
    "            self.acceptance[i] = defaultdict(list)\n",
    "\n",
    "            for _ in range(m):\n",
    "                \n",
    "                # compute alpha\n",
    "                alpha    = np.exp(log_alpha) + 1e-8              # numerical stability as alpha needs to be greater than 0\n",
    "\n",
    "                # sample the j-th parameter to update\n",
    "                j        = stats.dirichlet(alpha).rvs().argmax() # draws m numbers but select the one with the highest value\n",
    "\n",
    "                # copy most recent theta baseline\n",
    "                theta    = theta_baseline.copy()\n",
    "\n",
    "                # sample the j-th element\n",
    "                theta[j] = self.transition(theta[j], j)\n",
    "                \n",
    "                # compute log_pstar for theta\n",
    "                logp     = self.log_pstar(theta, self.data, **self.kwargs)\n",
    "\n",
    "                # accept with probability p_new / p_old\n",
    "                if np.log(np.random.uniform()) < (logp - logp_baseline):\n",
    "                    \n",
    "                    # increment the j-th element of log_alpha by the improvement in log_pstar value\n",
    "                    log_alpha[j]  += logp - logp_baseline\n",
    "\n",
    "                    theta_baseline = theta\n",
    "                    logp_baseline  = logp\n",
    "                    self.acceptance[i][j].append(True)\n",
    "                    \n",
    "                # reject the new sample\n",
    "                else:\n",
    "                    self.acceptance[i][j].append(False)\n",
    "                \n",
    "                # for numerical stability\n",
    "                log_alpha -= log_alpha.max()\n",
    "\n",
    "                # pull all values towards 0 (this prevents exploding values)\n",
    "                log_alpha *= self.rate\n",
    "                \n",
    "            self.samples[i]    = theta_baseline\n",
    "            self.log_pstars[i] = logp_baseline\n",
    "            \n",
    "            if verbose:\n",
    "                message.print(i, self.log_pstars[i], '' if i % verbose else '\\n')\n",
    "                \n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "X = np.random.normal(size = (100, 5))\n",
    "w = np.random.normal(scale = 3, size = 5)\n",
    "b = np.random.normal(scale = 10)\n",
    "\n",
    "y = X @ w + b + np.random.normal(scale = 0.2, size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_pstar(theta, data, **kwargs):\n",
    "    X, y = data\n",
    "    b, w = theta[0], theta[1:]\n",
    "    return stats.norm(X @ w + b, 0.2).logpdf(y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration | log pstar\n",
      "----------+----------\n",
      "        0 | -6.52e+04\n",
      "      100 | -3.78e+03\n",
      "      200 | -7.67e+01\n",
      "      300 | -7.38e+01\n",
      "      400 | -4.50e+01\n",
      "      500 | -2.49e+01\n",
      "      600 | -2.49e+01\n",
      "      700 | -2.49e+01\n",
      "      800 | -2.49e+01\n",
      "      900 | -2.49e+01\n",
      "    1,000 | -2.49e+01\n",
      "[-5.89406933  1.16883667 -0.20996651  3.22307695 -0.78225874 -0.91443858] [-5.81268477  1.14819729 -0.10272684  3.28904054 -0.7026474  -1.04235196]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21354336994483278"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta0    = np.zeros(6)\n",
    "deviation = np.ones(6) * 0.05\n",
    "data      = (X, y)\n",
    "\n",
    "mcmc = MetropolisHastingsSampler(log_pstar, deviation, data)\n",
    "\n",
    "mcmc.fit(1000, theta0)\n",
    "\n",
    "# compare the average of the samples to the true values\n",
    "print(mcmc.get_samples(500).mean(axis = 0), np.append(b, w))\n",
    "\n",
    "# norm distance\n",
    "np.linalg.norm(mcmc.get_samples(500).mean(axis = 0) - np.append(b, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration | log pstar\n",
      "----------+----------\n",
      "        0 | -6.52e+04\n",
      "      100 | -2.31e+04\n",
      "      200 | -4.41e+03\n",
      "      300 | -7.84e+01\n",
      "      400 | +1.65e+01\n",
      "      500 | +1.57e+01\n",
      "      600 | +1.42e+01\n",
      "      700 | +1.65e+01\n",
      "      800 | +1.70e+01\n",
      "      900 | +1.48e+01\n",
      "    1,000 | +1.61e+01\n",
      "[-5.85782613  1.13758115 -0.11418702  3.27399197 -0.71067281 -1.0822089 ] [-5.81268477  1.14819729 -0.10272684  3.28904054 -0.7026474  -1.04235196]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06450754387547969"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcmc = GibbsSampler(log_pstar, deviation, data)\n",
    "\n",
    "mcmc.fit(1000, theta0)\n",
    "\n",
    "# compare the average of the samples to the true values\n",
    "print(mcmc.get_samples(500).mean(axis = 0), np.append(b, w))\n",
    "\n",
    "# norm distance\n",
    "np.linalg.norm(mcmc.get_samples(500).mean(axis = 0) - np.append(b, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration | log pstar\n",
      "----------+----------\n",
      "        0 | -6.52e+04\n",
      "      100 | -3.85e+03\n",
      "      200 | +1.62e+01\n",
      "      300 | +1.33e+01\n",
      "      400 | +1.55e+01\n",
      "      500 | +1.50e+01\n",
      "      600 | +1.43e+01\n",
      "      700 | +1.43e+01\n",
      "      800 | +1.59e+01\n",
      "      900 | +1.25e+01\n",
      "    1,000 | +1.61e+01\n",
      "[-5.86363205  1.13474773 -0.11487291  3.26878705 -0.71104131 -1.08056184] [-5.81268477  1.14819729 -0.10272684  3.28904054 -0.7026474  -1.04235196]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06974740087091971"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcmc = AdaptiveGibbsSampler(log_pstar, deviation, data)\n",
    "\n",
    "mcmc.fit(1000, theta0)\n",
    "\n",
    "# compare the average of the samples to the true values\n",
    "print(mcmc.get_samples(500).mean(axis = 0), np.append(b, w))\n",
    "\n",
    "# norm distance\n",
    "np.linalg.norm(mcmc.get_samples(500).mean(axis = 0) - np.append(b, w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa841b4197c6a393f3c57e5da69b1046a13d3ffb1b2ced83c8f3efb4d967ec88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
